# List of cloud hosts for inference and fine-tuning

# Inference
- [banana.dev](https://www.banana.dev/): Hosting focused on high-throughput inference. [Deploys](https://www.banana.dev/blog/how-to-deploy-mistral-7b) from a GitHub repo.
- [fal.ai](https://www.fal.ai/): Hosting for inference, image generation and audio generation with [playgrounds](https://www.fal.ai/models) and shared endpoints for open source models.
- [replicate.com](https://replicate.com/): Lots of preconfigured models and playgrounds for text, image, music and speech generation.
- [lepton.ai](https://www.lepton.ai/): Hosting and inference via CLI tools. Can deploy prebuilt HuggingFace models with a single command.
- [titanml.co](https://titanml.co/): Inference deployements focused on CLI and enterprise.
    
# Inference and fine-tuning
- [anyscale.com](https://www.anyscale.com/endpoints): Inference and fine-tuning. Shared endpoints for Llama 2, Code Llama and Mistral 7B. Supports a common fine-tuning API with OpenAI.
- [together.ai](https://www.together.ai/): Inference and fine-tuning. [68 shared endpoints](https://api.together.xyz/playground) and playgrounds for text and image generation.
- [brev.dev](https://brev.dev/): Focused on instance hosting and CLI tools. Supports deploying to your existing AWS and GCP accounts.
- [gradient.ai](https://gradient.ai/): Focused on fine-tuning via CLI, Python, NodeJS and an online console to create and manage jobs.
- [fireworks.ai](https://fireworks}.ai/): Inference platform focused on CLI with a number of [shared endpoints](https://app.fireworks.ai/models). Supports LoRA addons.
